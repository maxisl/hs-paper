%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND AND RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Related Work}
\label{sec:background-and-related-work}

This section provides an overview over fundamental concepts that are necessary to
facilitate comprehensibility of all latter parts.
We start with explaining Large Language Models in general, followed by more specific explanations
of distinct prompting approaches.
These explanations are followed by a more general overview of related work on the subject.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LLMs %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Large Language Models (LLMs)}
\label{subsec:large-language-models-(llms)}
% TODO too repetitive?
One of the most widely used applications areas of generative AI are Large Language Models (LLMs).
Among LLMs, the most widely adopted is ChatGPT~\cite{openai_chatgpt_2023}, which is a
conversational model being developed by OpenAI\@.
The model is currently publicly accessible and free of charge.

% What is an LLM? What does it look like?
LLMs can be leveraged for a variety of tasks, but their main focus area is Natural Language Processing
(NLP).
Therefore, most LLMs designed for end users are implemented in the form of chatbots,
as is the case with ChatGPT for example.
They typically consist of an interface comprised of an input field for the user to type in arbitrary
text, as well as an output section that displays generated responses of the model.

% How does an LLM / ChatGPT work?
Large Language Models are a recent advancement that followed the development of the original
transformer architecture, which is a deep learning approach first introduced by researchers in 2017~\cite{vaswani_attention_2017}.
In future iterations, the Generative Pre-Training (GPT)~\cite{radford_improving_2018} approach
was adapted for text-based models in particular, laying the foundation for today's most
popular conversational LLMs, such as ChatGPT\@.
Since our research revolves around user interaction with dialog-focused models, we will not go into
more detail about other application and development areas of LLMs from hereon.
In addition to ChatGPT, there are also a variety of similar other models focused on text generation,
such as LaMDA~\cite{thoppilan_lamda_2022}, Sparrow~\cite{glaese_improving_2022}, or
BlenderBot 3~\cite{shuster_blenderbot_2022}.


% Why are LLMs important for our work? How do they come into play?
Large Language Models are a central part of our research as we investigate user behavior in LLM
conversations.
Currently, there are no binding guidelines on the concrete structure when prompting such models,
and users are therefore completely unconfined in their way of interacting with them.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ZERO-SHOT LEARNING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Zero-Shot Learning} % TODO Zero-Shot prompting instead of learning?
It is important to differentiate possible forms of prompting that exist in LLM interactions in order
to be able to correctly analyze and assess prompts in later parts of this paper.
To do so, we will clarify the terms \("\)Zero-Shot-\("\) as well as \("\)Few-Shot Learning\("\).
Zero-Shot Learning has been described as \("\)[\ldots] learning how to recognise
new concepts by just having a description of them\("\)\cite[p. 1]{feris_embarrassingly_2015}.

In regard to prompting, Zero-Shot refers to any form of prompt where the user does not specify
a particular example as part of the query~\cite[p. 1]{dang_how_2022}.
An example for such a query could be \("\)Give me five examples of papers that deal with the topic
of LLMs\("\).
As expected, model performance (accuracy) was measured as inferior in zero-shot prompting settings compared
to other settings where more information is provided in the prompt, such as a Few-Shot approach~\cite[p. 5]{brown_language_2020}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FEW-SHOT LEARNING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Few-Shot Learning}
In Few-Shot Learning in contrast, examples are given, albeit not many.
It is generally aimed at ensuring good learning performance with only a few (most often less than 20)
supervised examples~\cite[p. 1]{samuel_offline_2022}. % TODO word repetition: example
In a Few-Shot prompting setting, the user accordingly does provide increased guidance for the model.
Doing so can help improve model outputs~\cite[p. 1]{dang_how_2022}.
Giving one demonstration in the prompt is commonly referred to as a \("\)One-Shot\("\) approach~\cite[p. 6]{brown_language_2020},
whereas multiple make it \("\)Few-Shot\("\).
To exemplify the learning capabilities, we present an example inspired by the approach of Brown et al\(.\)~\cite{brown_language_2020}
and recent online resources~\cite{dairai_few-shot_2023} which displays our interaction with
the current ChatGPT-3 model in Table~\ref{tab:odd_number_sum}:

% TODO fix table layout
\documentclass[sigconf]{acmart}

\begin{document}
    \begin{table}
        \centering
        \caption{Odd Number Sum Evaluations}
        \begin{tabular}{p{0.1\linewidth}p{0.9\linewidth}}
            \toprule
            Prompt: & The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\ \\
            \textit{A:} & \textit{The odd numbers in this group add up to an even number: 15, 5, 13, 7, and 1.
            Their sum is 41, which is indeed an even number.}\\
            \midrule
            Prompt: & The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
            The answer is False. \\
            & The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
            The answer is True. \\
            & The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
            The answer is True. \\
            & The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
            The answer is False. \\
            & The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
            \\ \\
            \textit{A:} & \textit{The answer is False.
            The odd numbers in this group (15, 5, 13, 7, and 1) add up to 41, which is an odd number,
            not an even number.}\\
            \bottomrule
        \end{tabular}
        \label{tab:odd_number_sum}
    \end{table}
\end{document}



% TODO could also include example prompt with sentence completion, depending on space left

Using this example prompt, we could recreate the effects observed by other researchers.
We can observe that the request of the user is indeed first wrongly answered by the model
using a zero-shot approach.
Only when relying on few-shot prompting and supplying ChatGPT-3 with additional information,
the model generates the correct result.

We should note however, that ChatGPT is a dedicated language model, and we have used ChatGPT-3.
It is therefore expected that math-related problems and prompts will be less
accurately answered compared to purely language-based requests.
Depending on the model training it is imaginable that future iterations, such as ChatGPT-4,
will presumably improve performance and may therefore not have to rely on few-shot assistance as much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% USER INTERACTION WITH LLMS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% / RELATED WORK
\subsection{User Interaction with LLMs}
\label{subsec:user-interaction-with-llms}
Since our work investigates user interactions with LLMs in more detail, we want to provide a more detailed
overview of related work.

\subsubsection{Effectiveness of Query Reformulation}
% Paper: Analyzing and evaluating query reformulation strategies in web search logs
%

\subsubsection{Few-Shot Learning Capabilities}

\subsubsection{Challenges in Non-expert Prompt Design}
% Paper: Why Johnny can't Prompt...
% What are the issues ordinary users currently face?

\subsubsection{Opportunities and Challenges for Interactive Prompt Design Applications}
% Paper: How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning...
% "we propose four design goals for user interfaces that support prompting"

% What ideas are out there to improve prompt design?

\subsubsection{Human - AI Co-Authoring}
% Paper: CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities



% provide more general overview of subject

There