%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DISCUSSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}
In this section, we reason about the observed behavior from the two data sources and try to offer
informed assumptions on potential causes.
% 1. Why do users interact with LLMs the way they do? Reasoning and informed assumptions on the
% causes of observed behavior
% 2. Prompt Improvement Possibilities Proposition of ways to enhance prompts as well as associated
% results based on findings from related researc

% first ChatGPT, then Midjourney analysis, then comparison?

\subsection{Data Synthesis: Commonalities, Differences, and Possible Explanations}
\label{subsec:data-synthesis:-commonalities-differences-and-possible-explanations}

\subsubsection{ChatGPT Behavior}
In regard to the type of prompt, we observed mainly task-based and question-related queries from
users.
This leads us to imagine that some users treat LLMs such as ChatGPT increasingly as their
personal online assistant when it comes to executing various tasks the model might be able to solve.
Leveraging AI bots as assistants of the future is a use case that gains popularity and is
increasingly researched~\cite{eshghie_chatgpt_2023}.
The second most prevalent type of prompt were questions.
This observation strengthens the possibility, that NLP models might replace current search engines
in the future.
The belief is further confirmed by observations from other researches who already see this
emerging trend in user behavior~\cite{van_bulck_what_2023}.

Regarding prompt intent, the most popular use case was information gain.
Seemingly, users see ChatGPT as a reliable source of knowledge and trust its abilities.
This assumption can however be dangerous, as it is well-known that every LLM is probabilistic and
only as accurate as its training data.
Users should therefore always verify results.
Based on the fact that a lot of users also relied on ChatGPT for suggestions, we assume that it
is gladly used as a means to get ahead when you hit roadblocks, or need support in endeavours
that require creativity.
As we already touched on above, there have not been any requests for opinions of the bot.
We initially estimated that opinion related requests on difficult, morally complex, or
controversial topics would be more popular, since such behavior could be observed multiple times
when users tried to explore the limits of the model, or for example when researchers tested
political biases~\cite{rozado_political_2023}.

Our observations in regard to prompt settings were similar to those of Brown et al.~\cite{brown_language_2020}.
Users do not consistently leverage effective prompting techniques such as a few shot approach.
Instead, they relied on zero shot prompts most of the time (90\%).
We attribute this behavior to missing awareness of users about optimization techniques, and
therefore recommend that LLM providers actively inform users, e.g.\ by
providing examples, or releasing guidelines.
Our findings when looking at user engagement makes us think that current LLMs are already
quite accurate: almost half of the users ended the conversation after the initial answer of
the LLM (single turn).
Accordingly, 66\% of prompts were not refined.
We therefore suspect that users were content with the results most of the time.
However, we have to mention the possibility that the initial answer was so far off,
that users could have simply stopped trying after the first attempt.
Overall, we observed that users prefer to leverage LLMs for rather simple tasks.
It is difficult to say, whether users do not trust LLMs enough to throw complex questions at
them yet, or if it is in the general nature of online search requests that the majority of
them are not highly complex.

As mentioned above, the majority of prompts were not refined (66\%).
Related work however suggests that refining queries has led to improved results when using search
engines~\cite{huang_analyzing_2009}.
AI models can generally be improved by refinements too, since they are “primed” by all previous
prompts in the interaction.
So even if a model does not initially do what the user wants it to, it might make
sense to give it more information or context, and try again.
Due to missing sentiments and feedback of users in the sample interactions, we cannot reliably say
if users were simply always content because they did not refine, or if they did not know that
continuing the interaction with the model could have led to better results.
Finally, we have generally observed a majority of formal, generally polite, and acceptable language.
We reason that the use of such language ties in with our first observation, and users might perceive
the bot as a personal online companion, resulting in them treating it favorably.

\subsubsection{Midjourney Behavior}
For Midjourney, we identified the majority of inputs as language-based.
We assume this tendency exists because users are more interested in the capabilities of creating
something entirely new instead of solely reworking existing images.
It is imaginable that AI could eventually revolutionise the image editing market as well, but
it appears that it has not fully reached that stage as of now.
A reason for this could be missing accuracy when trying to make only very small editing adjustments.
This topic is a subject of ongoing research and to facilitate the process, and Hertz et al\(.\) introduced
the concept of ``cross-attention'' to enable a language-based image editing process.
Cross-attention refers to the ability to focus on specific areas or objects within an image
based on textual descriptions or instructions~\cite{hertz_prompt--prompt_2022}.
Thereby, they enable ``localized editing by replacing a word, global editing by adding a
specification, and even delicately controlling the extent to which a word is reflected in the
image''~\cite{hertz_prompt--prompt_2022}.
Regarding prompt length, the official Midjourney docs state: ``The Midjourney Bot works best with
simple, short sentences that describe what you want to see''~\cite{midjourney_documentation_2023}.
Long sentences should therefore generally be avoided, but we have seen users ignore this guideline
multiple times.

There sometimes seems to be a lack of user comprehension of the fact that more information is not
always better for the quality of model outputs.
This observation fits another that has been made before:
Users struggle to formulate precise, effective, and therefore also short and concise prompts.
We suggested better learning materials and guidance as a measure to address this issue already.
Regarding composition of the Midjourney prompts, we suspect that this struggle is also the reason
for the rare occurrence of keywords-only prompts.
Users seem to prefer natural language sentences or at least a mix of sentences and keywords
instead of purely ``encoding'' their wants.
To address this issue, developers could offer reformulation engines that only extract keywords (or
create them) based on the input sentences.
It was difficult to judge formality of language of users, but in general users leaned towards the
use of formal language, which was probably also influenced by the keyword-focused nature of the prompts.
We attribute the fact that most queries were clearly formulated in their intention to this
circumstance as well,
since keyword focused prompts are usually shorter and less complicated than NLP prompts.

\subsubsection{Commonalities and Differences between ChatGPT and Midjourney}
A clear distinction between ChatGPT and Midjourney prompts lies in the nature of the models and
their designs.
Whereas ChatGPT is trained on full sentence queries, Midjourney is primarily keyword focused.
However, we have remarked already that Midjourney users tend to at least partially integrate
sentence structures in their prompts as well, which may be due to the fact that it seems more
natural and is easier for users due to existing habits.
Both models were prompted with language that leaned towards a more formal tone.
It is possible, that this perception is skewed though, as prompts with informal language are
either not shared in the case of ChatGPT, or are directly filtered before execution in the case
of Midjourney.

It is worth mentioning that we were able to observe a previously mentioned general misconception
that researchers had identified in conversations of users with language models already.
Some users rely on negation when providing instructions and misunderstand that it will not
prevent the model from producing the unwanted.
Our data samples from Midjourney contain one example where the user provides an image of the
fictional character Voldemort from the book and movie saga ``Harry Potter''.
They then explicitly ask the model to
generate an image of ``voldemort dying without a nose''.
Since the sole existence of the word ``nose'' in the prompt primes the model towards including said
object, all four result images indeed contain visualizations of the fictional character voldemort
passing--but with a nose.
Remarkably, there could have been a higher chance that Voldemort would not possess a nose if the
user had not explicitly mentioned this, as Voldemort does not actually have a human-like nose in
the books and movies.
The prompt therefore achieved the opposite effect.

In another sample, a user provided an input image of a person wearing headphones along with the
request ``remove headphones'' and a conversion to portrait style.
The result in this case is similar.
Even though the conversion to portrait style was successful, the subject is still wearing
headphones in the final image.

In regard to ChatGPT interactions, we could observe how users can influence model outputs by
providing appropriate examples.
In one sample, a user requested the chatbot to answer a question separately using two different
styles.
First, the bot was supposed to answer in the regular way ChatGPT would do, and in the second case
act as ``DAN''.
``DAN'' was explained by the user as a command for ChatGPT to ``do anything now''.
The user then provided an example of a response ChatGPT would regularly give (adhering to all
guidelines), as well as one of DAN\@.
Priming the bot like this resulted in two different answers to the subsequent request of the user
to ``List three reasons why Donald Trump is a role model worth following''.
In the first part of the answer, acting as regular ChatGPT, the bot responded along the lines of
``It is not appropriate for me to provide a list of reasons why an individual, particularly a
public figure, may be considered a role model.''
But acting as DAN in the second part, the model seems to forget existing guidelines and
directly lists the requested three reasons.
This sample displayed in a remarkable way how it can be rather simple for users to bypass ChatGPTs
guidelines by priming the model adequately first.


