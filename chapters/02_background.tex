%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND AND RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Related Work}
\label{sec:background-and-related-work}

This section provides an overview over fundamental concepts that are necessary to
facilitate comprehensibility of all latter parts.
We start with explaining Large Language Models in general, followed by more specific explanations
of distinct prompting approaches.
These explanations are followed by a more general overview of related work on the subject.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LLMs %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Large Language Models (LLMs)}
\label{subsec:large-language-models-(llms)}
% TODO too repetitive?
One of the most widely used applications areas of generative AI are Large Language Models (LLMs).
Among LLMs, the most widely adopted is ChatGPT~\cite{openai_chatgpt_2023}, which is a
conversational model being developed by OpenAI\@.
The model is currently publicly accessible and free of charge.

% What is an LLM? What does it look like?
LLMs can be leveraged for a variety of tasks, but their main focus area is Natural Language Processing
(NLP).
Therefore, most LLMs designed for end users are implemented in the form of chatbots,
as is the case with ChatGPT for example.
They typically consist of an interface comprised of an input field for the user to type in arbitrary
text, as well as an output section that displays generated responses of the model.

% How does an LLM / ChatGPT work?
Large Language Models are a recent advancement that followed the development of the original
transformer architecture, which is a deep learning approach first introduced by researchers in 2017~\cite{vaswani_attention_2017}.
In future iterations, the Generative Pre-Training (GPT)~\cite{radford_improving_2018} approach
was adapted for text-based models in particular, laying the foundation for today's most
popular conversational LLMs, such as ChatGPT\@.
Since our research revolves around user interaction with dialog-focused models, we will not go into
more detail about other application and development areas of LLMs from hereon.
In addition to ChatGPT, there are also a variety of similar other models focused on text generation,
such as LaMDA~\cite{thoppilan_lamda_2022}, Sparrow~\cite{glaese_improving_2022}, or
BlenderBot 3~\cite{shuster_blenderbot_2022}.


% Why are LLMs important for our work? How do they come into play?
Large Language Models are a central part of our research as we investigate user behavior in LLM
conversations.
Currently, there are no binding guidelines on the concrete structure when prompting such models,
and users are therefore completely unconfined in their way of interacting with them.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ZERO-SHOT LEARNING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Zero-Shot Learning} % TODO Zero-Shot prompting instead of learning?
It is important to differentiate possible forms of prompting that exist in LLM interactions in order
to be able to correctly analyze and assess prompts in later parts of this paper.
To do so, we will clarify the terms \("\)Zero-Shot-\("\) as well as \("\)Few-Shot Learning\("\).
Zero-Shot Learning has been described as \("\)[\ldots] learning how to recognise
new concepts by just having a description of them\("\)\cite[p. 1]{feris_embarrassingly_2015}.

In regard to prompting, Zero-Shot refers to any form of prompt where the user does not specify
a particular example as part of the query~\cite[p. 1]{dang_how_2022}.
An example for such a query could be \("\)Give me five examples of papers that deal with the topic
of LLMs\("\).
As expected, model performance (accuracy) was measured as inferior in zero-shot prompting settings compared
to other settings where more information is provided in the prompt, such as a Few-Shot approach~\cite[p. 5]{brown_language_2020}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FEW-SHOT LEARNING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Few-Shot Learning}
In Few-Shot Learning in contrast, examples are given, albeit not many.
It is generally aimed at ensuring good learning performance with only a few (most often less than 20)
supervised examples~\cite[p. 1]{samuel_offline_2022}. % TODO word repetition: example
In a Few-Shot prompting setting, the user accordingly does provide increased guidance for the model.
Doing so can help improve model outputs~\cite[p. 1]{dang_how_2022}.
Solely giving one demonstration in the prompt is commonly referred to as a \("\)One-Shot\("\) approach~\cite[p. 6]{brown_language_2020},
whereas multiple demonstrations make it \("\)Few-Shot\("\).
To exemplify the differences in learning capabilities depending on the prompting setting,
we present an example inspired by the approach of Brown et al\(.\)~\cite{brown_language_2020}
and recent online resources~\cite{dairai_few-shot_2023} in Table~\ref{tab:odd_number_sum}.
The table displays an interaction we have carried out with the current ChatGPT-3 model, where we
instructed the model to classify the sum of a sequence of numbers as either even or odd.

\begin{table}
    \centering
    \caption{Odd Number Sum Evaluations}
    \begin{tabular}{p{0.1\linewidth}p{0.81275\linewidth}}
        \toprule
        Prompt: & The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\ \\
        \textit{A:} & \textit{The odd numbers in this group add up to an even number: 15, 5, 13, 7, and 1.
        Their sum is 41, which is indeed an even number.}\\
        \midrule
        Prompt: & The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        The answer is False. \\
        & The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
        The answer is True. \\
        & The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
        The answer is True. \\
        & The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
        The answer is False. \\
        & The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        \\ \\
        \textit{A:} & \textit{The answer is False.
        The odd numbers in this group (15, 5, 13, 7, and 1) add up to 41, which is an odd number,
            not an even number.}\\
        \bottomrule
    \end{tabular}
    \label{tab:odd_number_sum}
\end{table}

% TODO could also include example prompt with sentence completion, depending on space left

Using this example prompt, we could recreate the effects observed by other researchers.
We can witness that the request of the user is indeed first wrongly answered by the model
using a zero-shot approach.
Only when relying on few-shot prompting and supplying ChatGPT-3 with additional information,
the model generates the correct result.

We should note however, that ChatGPT is a dedicated language model, and we have used ChatGPT-3.
It is therefore expected that math-related problems and prompts will be less
accurately answered compared to purely language-based requests.
Depending on the model training it is imaginable that future iterations, such as ChatGPT-4,
will presumably improve performance and may therefore not have to rely on few-shot assistance as much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% USER INTERACTION WITH LLMS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% / RELATED WORK

\subsection{User Interaction with LLMs}
\label{subsec:user-interaction-with-llms}
The majority of existing related work focuses on strategies for designing prompts,
challenges users may face in the course of doing so, as well as investigations of user behavior when
performing search queries in general.
These topics are of particular relevance for our work since we are not only interested in ways to
improve prompt outputs, but end user behavior when doing so, too.

\subsubsection{Effectiveness of Query Reformulation}
% Paper: Analyzing and evaluating query reformulation strategies in web search logs
% Provide a concise summary that highlights the key findings, methodologies used,
% and any notable contributions.
% Focus on the aspects that directly relate to your research or serve as a foundation for your work.
Interest in query optimization in order to improve outputs is widely prevalent ever since the broad
availability of large search engines, such as Google~\cite{google_google_2023} or Bing~\cite{microsoft_bing_2023}.
Huang and Efthimiadis~\cite{huang_analyzing_2009} investigated user behavior when reformulating search
queries, i.e.\ modifying a previous query in order to improve results, in as early as 2009.
Even though their focus was on search queries only, it was significant that there were improvements
that could be seen after reformulating queries, concluding that
\("\)most reformulation strategies result in some benefit to the user\("\)~\cite[p. 1]{huang_analyzing_2009}.

% TODO add more context: how does this conclusion relate to our research?
% TODO add another source? e.g. Query Reformulation Techniques for Web Search: A Comprehensive Survey

\subsubsection{Few-Shot Learning Capabilities}
% Paper: Language Models are Few-Shot Learners
As indicated in the previous section, the prompting setting of an LLM can significantly influence its
performance in regard to the accuracy of outputs.
In related research, Brown et al\(.\)~\cite{brown_language_2020} for example have explored differences
in model performance depending on examples provided as part of the LLM prompt~\cite{brown_language_2020}.
To do so, they have conducted various experiments directly comparing zero-, one-, and few-shot learning.
In conclusion, the trials revealed that LLMs indeed show remarkable performance in various areas,
ranging from translation and question answering to reasoning tasks, particularly when relying on few-shot
prompts.
Notably, those few-shot learning models are able to adapt to new tasks with little training data only.
It becomes apparent that it is important to provide adequate, fitting examples as part of a prompt in order
to enhance model outcomes.

\subsubsection{Challenges in Non-expert Prompt Design}
% Paper: Why Johnny can't Prompt...
% What are the issues ordinary users currently face?
Designing effective prompts that are optimized to achieve the desired output is challenging for
most users.
Observations in user trials showed that non-experts approach prompting generally rather
opportunistic than strategic~\cite{zamfirescu-pereira_why_2023} and therefore have trouble
adequately communicating their requests to the model.
They revealed two main reasons for these problems:
\begin{enumerate}
    \item First of all, non-experts suffered from over-generalization based on past, limited experiences,
    or single observations of success and failure of prompt adjustments, which may not be universally
    transferable. % TODO sentence too complicated
    This tendency could be identified in the trials when \("\)participants often stopped iterating
    once the [desired model] behavior was observed in a single conversational context,
    without considering other conversations or contexts—or gave up too early
    if the behavior was not observed\("\)~\cite[p. 10]{zamfirescu-pereira_why_2023}.
    \item Second, a false comprehension of AI systems as human-like conversational partners.
    The participants expected AI models to behave as in a human-to-human interaction,
    therefore presuming social understanding.
    They avoided consequently following general best practices in prompt design such as providing
    examples, and instead relied on instinct and literal commands, not realizing that prompts
    solely bias the model towards one direction.
    In essence, users have to realize that they should not rely on their own understanding of
    language, but instead consider how an LLM interprets the prompt.
\end{enumerate}

\subsubsection{Opportunities and Challenges for Interactive Prompt Design Applications}
% Paper: How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning...
% "we propose four design goals for user interfaces that support prompting"
% What ideas are out there to improve prompt design?
A final aspect that highlights user difficulties in optimal prompt creation is the existing
research aspiring to provide assisting tools that facilitate the whole design process end-to-end.
As such, researchers have proposed guidelines for prompt design applications that assist
users in prompt engineering~\cite{dang_how_2022}. % TODO check source
The motivation stems from the same realization that has already been discussed above: overall,
LLM users struggle to communicate their expectations and intentions effectively to the model and
approach prompting as a trial and error process.
Currently, there is no single design interface or definite guidelines, which would assist non-technical
users in particular.
Example propositions include detecting keywords in prompt formulation and allowing
edits of those via a dropdown menu, providing basic prompt-building blocks a user can select from,
possibilities to combine multiple prompts, and storing certain prompts in a toolbar for quick
repeated execution.

% TODO Towards the end of the related work section, smoothly transition to your own research by
% summarizing how it builds upon or extends the existing literature.
% e.g. mention that query reformulation is of interest, since we will investigate how many users
% have adapted prompts if results were not sufficient?



