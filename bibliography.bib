%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Maximilian Slapnik at 2023-05-24 18:28:37 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{radford_improving_2018,
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	date-added = {2023-05-24 18:28:22 +0200},
	date-modified = {2023-05-24 18:28:22 +0200},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/Users/maximilianslapnik/Zotero/storage/BNIVP4RK/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
	language = {en},
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	year = {2018}}

@article{vaswani_attention_2017,
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	date-added = {2023-05-24 18:15:01 +0200},
	date-modified = {2023-05-24 18:15:01 +0200},
	file = {Vaswani et al. - Attention is All you Need.pdf:/Users/maximilianslapnik/Zotero/storage/S7NUJBLF/Vaswani et al. - Attention is All you Need.pdf:application/pdf},
	language = {en},
	title = {Attention is {All} you {Need}},
	volume = {30},
	year = {2017}}

@inproceedings{lee_coauthor_2022,
	abstract = {Large language models (LMs) ofer unprecedented language generation capabilities and exciting opportunities for interaction design. However, their highly context-dependent capabilities are difcult to grasp and are often subjectively interpreted. In this paper, we argue that by curating and analyzing large interaction datasets, the HCI community can foster more incisive examinations of LMs' generative capabilities. Exemplifying this approach, we present CoAuthor, a dataset designed for revealing GPT-3's capabilities in assisting creative and argumentative writing. CoAuthor captures rich interactions between 63 writers and four instances of GPT-3 across 1445 writing sessions. We demonstrate that CoAuthor can address questions about GPT-3's language, ideation, and collaboration capabilities, and reveal its contribution as a writing ``collaborator'' under various defnitions of good collaboration. Finally, we discuss how this work may facilitate a more principled discussion around LMs' promises and pitfalls in relation to interaction design. The dataset and an interface for replaying the writing sessions are publicly available at https://coauthor.stanford.edu.},
	address = {New Orleans LA USA},
	author = {Lee, Mina and Liang, Percy and Yang, Qian},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	date-added = {2023-05-24 17:09:00 +0200},
	date-modified = {2023-05-24 17:09:00 +0200},
	doi = {10.1145/3491102.3502030},
	file = {Lee et al. - 2022 - CoAuthor Designing a Human-AI Collaborative Writi.pdf:/Users/maximilianslapnik/Zotero/storage/B58HKHBC/Lee et al. - 2022 - CoAuthor Designing a Human-AI Collaborative Writi.pdf:application/pdf},
	isbn = {978-1-4503-9157-3},
	language = {en},
	month = apr,
	pages = {1--19},
	publisher = {ACM},
	shorttitle = {{CoAuthor}},
	title = {{CoAuthor}: {Designing} a {Human}-{AI} {Collaborative} {Writing} {Dataset} for {Exploring} {Language} {Model} {Capabilities}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3502030},
	urldate = {2023-05-18},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3491102.3502030},
	bdsk-url-2 = {https://doi.org/10.1145/3491102.3502030}}

@misc{openai_chatgpt_2023,
	author = {{OpenAI}},
	date-added = {2023-05-24 16:48:23 +0200},
	date-modified = {2023-05-24 16:48:23 +0200},
	file = {ChatGPT | OpenAI:/Users/maximilianslapnik/Zotero/storage/SNJ44DY6/login.html:text/html},
	title = {{ChatGPT}},
	url = {https://chat.openai.com/auth/login},
	urldate = {2023-05-24},
	year = {2023},
	bdsk-url-1 = {https://chat.openai.com/auth/login}}
