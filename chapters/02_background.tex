%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND AND RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Related Work}
\label{sec:background-and-related-work}

This section provides an overview over fundamental concepts that are necessary to
facilitate comprehensibility of all latter parts.
We start with explaining Large Language Models in general, followed by more specific explanations
of distinct prompting approaches.
These explanations are followed by a more general overview of related work on the subject.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LLMs %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Large Language Models (LLMs)}
\label{subsec:large-language-models-(llms)}
% TODO too repetitive?
One of the most widely used applications areas of generative AI are Large Language Models (LLMs).
Among LLMs, the most widely adopted is ChatGPT~\cite{openai_chatgpt_2023}, which is a
conversational model being developed by OpenAI\@.
The model is currently publicly accessible and free of charge.

% What is an LLM? What does it look like?
LLMs can be leveraged for a variety of tasks, but their main focus area is Natural Language Processing
(NLP).
Therefore, most LLMs designed for end users are implemented in the form of chatbots,
as is the case with ChatGPT for example.
They typically consist of an interface comprised of an input field for the user to type in arbitrary
text, as well as an output section that displays generated responses of the model.

% How does an LLM / ChatGPT work?
Large Language Models are a recent advancement that followed the development of the original
transformer architecture, which is a deep learning approach first introduced by researchers in 2017~\cite{vaswani_attention_2017}.
In future iterations, the Generative Pre-Training (GPT)~\cite{radford_improving_2018} approach
was adapted for text-based models in particular, laying the foundation for today's most
popular conversational LLMs, such as ChatGPT\@.
Since our research revolves around user interaction with dialog-focused models, we will not go into
more detail about other application and development areas of LLMs from hereon.
In addition to ChatGPT, there are also a variety of similar other models focused on text generation,
such as LaMDA~\cite{thoppilan_lamda_2022}, Sparrow~\cite{glaese_improving_2022}, or
BlenderBot 3~\cite{shuster_blenderbot_2022}.


The primary method of interacting with LLMs is the previously mentioned approach of \("\)prompting\("\) the model.
Currently, there are no binding guidelines on the concrete structure of prompts,
and users are therefore completely unconfined in their way of interacting.
In the following, we go into more detail about different types of prompts, namely zero- and few
-shot.

% TODO include subsubsection "prompting" and explain the general concept?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ZERO-SHOT LEARNING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Zero-Shot Learning} % TODO Zero-Shot prompting instead of learning?
It is important to differentiate possible forms of prompting that exist in LLM interactions in order
to be able to correctly analyze and assess prompts in later parts of this paper.
To do so, we will clarify the terms \("\)Zero-Shot-\("\) as well as \("\)Few-Shot Learning\("\).
Zero-Shot Learning has been described as \("\)[\ldots] learning how to recognise
new concepts by just having a description of them\("\)\cite[p. 1]{feris_embarrassingly_2015}.

In regard to prompting, Zero-Shot refers to any form of prompt where the user does not specify
a particular example as part of the query~\cite[p. 1]{dang_how_2022}.
An example for such a query could be \("\)Give me five examples of papers that deal with the topic
of LLMs\("\).
As expected, model performance (accuracy) was measured as inferior in zero-shot prompting settings compared
to other settings where more information is provided in the prompt, such as a Few-Shot approach~\cite[p. 5]{brown_language_2020}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FEW-SHOT LEARNING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Few-Shot Learning}
In Few-Shot Learning in contrast, examples are given, albeit they do not necessarily have to be
many.
It is generally aimed at ensuring good learning performance with only a few (most often less than 20)
supervised examples~\cite[p. 1]{samuel_offline_2022}. % TODO word repetition: example
In a Few-Shot prompting setting, the user accordingly does provide increased guidance for the model.
Doing so can help improve model outputs~\cite[p. 1]{dang_how_2022}.
Solely giving one demonstration in the prompt is commonly referred to as a \("\)One-Shot\("\) approach~\cite[p. 6]{brown_language_2020},
whereas multiple demonstrations make it \("\)Few-Shot\("\).
To exemplify the differences in learning capabilities depending on the prompting setting,
we present an example inspired by the approach of Brown et al\(.\)~\cite{brown_language_2020}
and recent online resources~\cite{dairai_few-shot_2023} in Table~\ref{tab:odd_number_sum}.
The table displays an interaction we have carried out with the current ChatGPT-3 model, where we
instructed the model to classify the sum of a sequence of numbers as either even or odd.

\begin{table}
    \centering
    \caption{ChatGPT: Odd Number Sum Evaluations}
    \begin{tabular}{p{0.1\linewidth}p{0.81275\linewidth}}
        \toprule
        Prompt: & The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \\ \\
        \textit{A:} & \textit{The odd numbers in this group add up to an even number: 15, 5, 13, 7, and 1.
        Their sum is 41, which is indeed an even number.}\\
        \midrule
        Prompt: & The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        The answer is False. \\
        & The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
        The answer is True. \\
        & The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
        The answer is True. \\
        & The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
        The answer is False. \\
        & The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        \\ \\
        \textit{A:} & \textit{The answer is False.
        The odd numbers in this group (15, 5, 13, 7, and 1) add up to 41, which is an odd number,
            not an even number.}\\
        \bottomrule
    \end{tabular}
    \label{tab:odd_number_sum}
\end{table}

% TODO could also include example prompt with sentence completion, depending on space left

Using this example prompt, we could recreate the effects observed by other researchers.
We can witness that the user request is indeed first wrongly answered by the model
using a zero-shot approach.
Only when relying on few-shot prompting and supplying ChatGPT-3 with additional information,
the model generates the correct result.

We should note however, that ChatGPT is a dedicated language model, and we have used ChatGPT-3.
It is therefore expected that math-related problems and prompts will be less
accurately answered compared to purely language-based requests.
Depending on the model training and technological progress, it is imaginable that future
iterations, such as ChatGPT-4, will presumably improve performance and may therefore not have to
rely on few-shot assistance as much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% USER INTERACTION WITH LLMS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% / RELATED WORK

\subsection{User Interaction with LLMs}
\label{subsec:user-interaction-with-llms}
The majority of existing related work focuses on strategies for designing prompts,
challenges users may face in the course of doing so, as well as investigations of user behavior when
performing search queries in general.
These topics are of particular relevance for our work since we are not only interested in ways to
improve prompt outputs, but also in end user behavior while prompting.

\subsubsection{Effectiveness of Query Reformulation}
% Paper: Analyzing and evaluating query reformulation strategies in web search logs
% Provide a concise summary that highlights the key findings, methodologies used,
% and any notable contributions.
% Focus on the aspects that directly relate to your research or serve as a foundation for your work.
Interest in query optimization in order to improve outputs is widely prevalent ever since the broad
availability of large search engines, such as Google~\cite{google_google_2023} or Bing~\cite{microsoft_bing_2023}.
Results of search query reformulations can be an indication to also look at prompt reformulation
and its effectiveness in more detail.
Huang and Efthimiadis~\cite{huang_analyzing_2009} investigated user behavior when reformulating search
queries, i.e.\ modifying a previous query in order to improve results, in as early as 2009.
Even though their focus was on search queries only, it was significant that there were improvements
that could be seen after reformulating queries, concluding that
\("\)most reformulation strategies result in some benefit to the user\("\)~\cite[p. 1]{huang_analyzing_2009}.

% TODO add more context: how does this conclusion relate to our research?
% TODO add another source? e.g. Query Reformulation Techniques for Web Search: A Comprehensive Survey

\subsubsection{Few-Shot Learning Capabilities}
% Paper: Language Models are Few-Shot Learners
As indicated in the previous section, the prompting setting of an LLM can significantly influence its
performance in regard to the accuracy of outputs.
In related research, Brown et al\(.\)~\cite{brown_language_2020} for example have explored differences
in model performance depending on examples provided as part of the LLM prompt~\cite{brown_language_2020}.
To do so, they have conducted various experiments directly comparing zero-, one-, and few-shot learning.
In conclusion, the trials revealed that LLMs indeed show remarkable performance in various areas,
ranging from translation and question answering to reasoning tasks, particularly when relying on few-shot
prompts.
Notably, those few-shot learning models are able to adapt to new tasks with little training data only.
It becomes apparent that it is important to provide adequate, fitting examples as part of a prompt in order
to enhance model outcomes.

\subsubsection{Challenges in Non-expert Prompt Design}
% Paper: Why Johnny can't Prompt...
% What are the issues ordinary users currently face?
Designing effective prompts that are optimized to achieve the desired output is challenging for
most users.
Observations in user trials showed that non-experts approach prompting generally rather
opportunistic than strategic~\cite{zamfirescu-pereira_why_2023} and therefore have trouble
adequately communicating their requests to the model.
The trials revealed two main reasons for these problems:
\begin{enumerate}
    \item First of all, non-experts suffered from over-generalization based on past, limited experiences,
    or single observations of success and failure of prompt adjustments, which may not be universally
    transferable. % TODO sentence too complicated
    This tendency could be identified in the trials when \("\)participants often stopped iterating
    once the [desired model] behavior was observed in a single conversational context,
    without considering other conversations or contexts—or gave up too early
    if the behavior was not observed\("\)~\cite[p. 10]{zamfirescu-pereira_why_2023}.
    \item Second, a false comprehension of AI systems as human-like conversational partners.
    The participants expected AI models to behave as in a human-to-human interaction,
    therefore presuming social understanding.
    They avoided consequently following general best practices in prompt design such as providing
    examples, and instead relied on instinct and literal commands, not realizing that prompts
    solely bias the model towards one direction.
    In essence, users have to realize that they should not rely on their own understanding of
    language, but instead consider how an LLM interprets the prompt.
    The example Zamfirescu et al\(.\) give in their research is the misconception of many users
    that simply prompting a model not to say \("\)XYZ\("\) will not actually prevent the model from doing
    so in any case.
    It is still possible that the model will output \("\)XYZ\("\) in subsequent responses due to
    the probabilistic nature of LLMs and the fact that even though responses can be primed
    by earlier prompts in the conversation, they are generated ad-hoc based on the most relevant
    training data of the model.
\end{enumerate}

\subsubsection{Opportunities and Challenges for Interactive Prompt Design Applications}
% Paper: How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning...
% "we propose four design goals for user interfaces that support prompting"
% What ideas are out there to improve prompt design?
A final aspect that highlights user difficulties in optimal prompt creation is the existing
research aspiring to provide assisting tools that facilitate the whole design process end-to-end.
As such, researchers have proposed guidelines for prompt design applications that assist
users in prompt engineering by providing an adequate interface~\cite{dang_how_2022}.
The motivation stems from the same realization that has already been discussed above: overall,
LLM users struggle to communicate their expectations and intentions effectively to the model.
Instead, they commonly approach prompting as a trial and error process.
Currently, there is no single design interface or definite guidelines for prompts, which would
assist non-technical users in particular.
Example propositions to ease design include detecting keywords in prompt formulation and allowing
edits of those via a dropdown menu, providing basic prompt-building blocks from which a user can
select,
offering the possibility to combine multiple prompts, and storing selected prompts in a toolbar for
quick and repeated execution. % TODO shorten?
\newline

In summary, the volume of related research in regard to user prompting behavior in LLM
interactions shows the significance of the topic for effective use of this technology.
Few definitive guidelines exist, and users are largely left on their own when formulating prompts,
especially non-technical users which may lack deep knowledge on the subject.
Although superior prompting strategies have been discovered, users do not consistently
implement them, even when specifically instructed to do so.
So far, these behaviors have been attributed to existing habits and missing perception of LLMs as
artificial and not human.
Since related work suggests that reformulating search queries is a popular strategy to improve
results, we want to investigate if users apply this strategy in LLM conversations as well by
looking at real-world examples. % TODO check if we really did this
Furthermore, we want to discover if users still show a lack of awareness of effective prompt
formulation strategies, such as few-shot learning, and if they rely on appropriate language that
is machine and not human directed, i.e.\ showing comprehension of the fundamental difference
between talking to a machine and a human.
