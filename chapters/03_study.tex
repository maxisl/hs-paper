%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% STUDY ON USAGE PATTERNS OF LLM USERS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Study on Usage Patterns of LLM Users}
\label{sec:study-on-usage-patterns-of-llm-users}

% Ideas for language prompt coding categories:
%- Prompt type: could include questions, statements, commands, or specific task-based prompts
%- User intent: could include seeking information, asking for advice, requesting clarification,
% expressing opinions, or making suggestions
%- Response quality: might include accurate and helpful responses, incomplete or ambiguous
% responses, irrelevant responses, or responses requiring further clarification
%- User satisfaction: could include satisfied, dissatisfied, neutral, confused, or impressed
%- Engagement level: might include active conversation, single-turn interactions, probing for
% more information, or exploratory questioning
%- Task success: could include successful completion, partial success, or failure
%- Response length
%- Ethical considerations: might include potential biases in the generated responses, adherence
% to ethical guidelines, or fairness and inclusivity in the LLM's behavior

% % Ideas for image prompt coding categories:
% Prompt type: capture the types of inputs users provide to the image generation models
% User intent: Extend the user intent category to encompass image-related intents--could
% include users requesting image generation, describing desired visual characteristics, or
% seeking specific visual outcomes.
% Response quality: evaluate the quality of the generated images--consider aspects such as visual
% fidelity, realism, relevance to the user's request, or adherence to given guidelines.
% User satisfaction: not possible? maybe on discord (but hard to discern)
% Engagement level: might involve exploring how users refine their requests, provide feedback on
% generated images, or iteratively interact with the model to achieve their desired visual
% outcomes. (hard to discern, same as above)
% Task success: might include successful image generation, partially successful
% results, or cases where the model failed to meet the user's expectations.
% Ethical considerations: could involve examining issues such as biases in generated images,
% ethical implications of content creation, privacy concerns related to user-provided images, or
% responsible use of image generation models.

% analysis: do we see differences between text and image prompts? which is more successful /
% accurate in general? do we observe sentiment differences?

% Analyze distribution of zero-, one-, and few-shot prompts
% analyze frequency of prompt reformulations in order to improve results
%

\subsection{Research Objective}
\label{subsec:research-objective}
% Intro and Research Objective of the study goal, the methodology, and the individual
% steps that will be taken
The main outlined goal of our research is to gain a fundamental understanding of user behavior in conversation with Large Language Models.
This analysis includes identifying common patterns and strategies in those interactions.
Previous research indicates that users regularly face challenges and difficulties, especially
when trying to formulate effective prompts.
Through accumulation and analysis of qualified data samples we aim to identify and understand these
challenges,
as well as investigate the impact and effect of user behavior on the effectiveness of LLM responses.
Given the various kinds of available generative models, we want to examine differences in
prompting behavior according to model type as well.

Since related insights suggest that reformulating search queries is a popular strategy to improve
results, we want to investigate if users apply this strategy in LLM conversations as well. % TODO
% check if we really did this
Furthermore, we want to assess the extent to which users show awareness of effective prompt
formulation strategies, such as few-shot learning, and whether they rely on appropriate language
that is machine and not human directed, thus showing comprehension of the fundamental difference
between talking to a machine versus a human.

\subsection{Research Method: ShareGPT and Midjourney}
\label{subsec:research-method:-sharegpt-and-midjourney}
% Information on the ShareGPT & Midjourney platforms, their user base, suitability for the
% study, and which data we are going to use
In order to obtain credible insights, we complement existing findings with real-world data.
Our study analyzes data samples from two different types of
LLMs.
First, we examine user interactions with ChatGPT, a generative NLP model, which has already been
described in more detail in Section~\ref{subsec:large-language-models-(llms)}.
These ChatGPT conversations were obtained from the website ShareGPT\cite{sharegpt_sharegpt_2023}.
ShareGPT is an open platform, that allows its community to publicly share interactions they have
had with the ChatGPT model.
As of today, ShareGPT has accumulated nearly 300.000 saved user conversations.
What makes ShareGPT particularly suitable for our use-case is the fact that the entire shared conversation
can be viewed by the observer as if they had personally conducted the interaction, allowing us to
gain a deeper understanding of the conversation dynamics and outcome.
\newline

Midjourney in contrast, is a platform that focuses on AI-based image generation.
Users can interact with the model through Discord\cite{} and submit individual requests. % TODO cite
In order to generate an illustration, users have to enter a descriptive prompt, similar to
ChatGPT\@.
The description typically includes everything that should appear in the picture, but may also
encompass the desired mood, drawing style, or composition of the image being generated.
Notably, the Midjourney Bot does not understand grammar, sentence structure, or specific words like
humans~\cite{}. % TODO cite
Midjourney's developers actively encourage using fewer, but more precise and impactful words when
prompting
the model.
For example, they suggest using \("\)gigantic\("\) instead of \("\)big\("\) in order to achieve
better results. %TODO cite
This recommendation stems from the fact that fewer words in a prompt intensify the influence each
individual word has on the final outcome.
However, it is important to mention that users have to strike a balance.
An adequate amount of precise words is mandatory, because anything that is not specified may be
randomized.
In addition to purely textual prompts, the platform allows image inputs as well.
Users may provide an image as a guideline or basis including instructions about things to modify,
add, remove, or remodel.
The Midjourney platform on Discord has experienced rapid growth, and counts more than 17 million
members as of today.
\newline

In order to verify observations and findings we have presented in Section~\ref{sec:background-and
-related-work},
we examine exemplary real-world user interaction samples in the following.
By randomly choosing 50 conversations from each the ShareGPT website as well as Midjourney's Discord
channel, we obtain a representative sample of average user behavior in both text and image targeting prompts.
We have defined dedicated categories according to which each sample is classified for both
the ChatGPT and Midjourney conversations.
For the language-focused ChatGPT conversations, the categories and specific sub-categories can be
seen in \ref{table}.
Similarly, the categories and sub-categories for image-focused prompts in Midjourney are listed
in \ref{table}.

For ShareGPT, we classified the prompt by type since in theory any kind of prompt is possible,
as users are solely constrained to natural language in any shape or form.
It was also of major interest what the user intended to achieve with their individual prompts,
i.e.\ what they use the model for.
Prompt length and setting refer to the number of sentences in the user inputs, and whether
any examples were provided as part of the query.
If the user prompted the model multiple times (at least twice) in the course of the conversation, we considered the interaction multi turn, otherwise single turn.
The prompt's complexity gave us an idea whether users leverage ChatGPT for simple
tasks, that they may otherwise quickly research using a search engine, or if they pose complex
questions that require expert-level knowledge.
The refinement degree of the prompt revealed if users were generally content with the initial
answer of the LLM, or if further elaboration was needed.
Finally, we differentiated use of formal and informal language.
When a single conversation consisted of multiple prompts, we classified based on the most
frequently observed category or significant behavior.

We classified Midjourney interactions using a similar approach.
First of all, we differentiated between image- and purely language-based inputs.
We then considered the length of the prompt, and whether it consisted solely of keywords, one
or more sentences, or a mix.
Next, we recorded the complexity of the whole prompt.
The Midjourney bot always generates four versions of the desired image.
It then allows users to either recreate variations or more detailed versions of one or more of
the those four results.
Users can also re-execute the whole generation process.
We thus classified the prompt accordingly in the refinement category.
Finally, we recorded the clarity of the prompt and satisfaction levels based
on the observed user behavior.
If a user created variations or more detailed versions of the result, we assumed they were
generally satisfied.
Analogously, we assumed dissatisfaction if they regenerated the whole image.

% set linewidth 0,2281875 for \begin{tabular}{p{0.1\linewidth}p{0.81275\linewidth}}
\begin{table}[]
    \centering
    \caption{ShareGPT Prompt Analysis Categories}
    \begin{tabular}{@{}llll@{}}
        \toprule
        Type        & Intent        & Length             & Setting   \\ \midrule
        Question    & Information   & Long (5 + Sent.)   & Zero Shot \\
        Statement   & Advice        & Med (2 - 4 Sent.)  & One Shot  \\
        Command     & Clarification & Short (<= 1 Sent.) & Few Shot  \\
        Task-based  & Opinion       &                    &           \\
        & Suggestion    &                    &           \\
        & Entertainment &                    &           \\
        &               &                    &           \\
        \toprule
        Engagement  & Complexity    & Refinement         & Language  \\ \midrule
        Single Turn & Simple        & None               & Formal    \\
        Multi Turn  & Intermediate  & Once               & Informal  \\
        &               & Complex            &           \\ \bottomrule
    \end{tabular}
    \label{tab:table}
\end{table}

\begin{table}[]
    \centering
    \caption{Midjourney Prompt Analysis Categories}
    \begin{tabular}{@{}llll@{}}
        \toprule
        Type         & Length            & Composition  & Complexity   \\ \midrule
        Language     & Long (12+ Words)  & Keyword-Only & Simple       \\
        Image        & Med (4-12 Words)  & Sentence     & Intermediate \\
        & Short (1-3 Words) & Mix          & Complex      \\
        \\
        \toprule
        Refinement          & Language & Clarity & Satisfaction \\
        \midrule
        None & Formal & Clear & Satisfied \\
        Variation          & Informal & Ambiguous & Dissatisfied \\
        Regeneration & & & Unclear \\
        \bottomrule
    \end{tabular}
    \label{tab:table2}
\end{table}

% TODO section or subsection?


\section{Study Results}
\label{sec:study-results}

\subsection{Findings}
\label{subsec:findings}
% Listing of the results of the study, potentially segregated into categories that can be defined
% in advance

\subsection{Observable Trends}
\label{subsec:observable-trends}
% Objective analysis of results with a particular focus on observable trends in user behavior and
% data patterns (including visualizations such as charts)

% TODO make an own subsection on difficulties of ordinary users in understanding how LLMs interpret
% natural language? in this subsec, we could focus on the example also displayed on midjourneys
% website: " If you ask for a party with “no cake,” your image will probably include a cake"
